{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml_collections in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: absl-py in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from ml_collections) (2.0.0)\n",
      "Requirement already satisfied: PyYAML in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from ml_collections) (6.0)\n",
      "Requirement already satisfied: six in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from ml_collections) (1.16.0)\n",
      "Requirement already satisfied: contextlib2 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from ml_collections) (21.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: flax in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.12 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (1.26.0)\n",
      "Requirement already satisfied: jax>=0.4.2 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (0.4.19)\n",
      "Requirement already satisfied: msgpack in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (1.0.7)\n",
      "Requirement already satisfied: optax in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (0.1.7)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (0.4.1)\n",
      "Requirement already satisfied: tensorstore in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (0.1.46)\n",
      "Requirement already satisfied: rich>=11.1 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (13.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (4.7.1)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from jax>=0.4.2->flax) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from jax>=0.4.2->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from jax>=0.4.2->flax) (1.11.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from rich>=11.1->flax) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from optax->flax) (2.0.0)\n",
      "Requirement already satisfied: chex>=0.1.5 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from optax->flax) (0.1.84)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from optax->flax) (0.4.19)\n",
      "Requirement already satisfied: etils[epath,epy] in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from orbax-checkpoint->flax) (1.5.2)\n",
      "Requirement already satisfied: nest_asyncio in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from orbax-checkpoint->flax) (1.5.8)\n",
      "Requirement already satisfied: protobuf in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from orbax-checkpoint->flax) (4.24.4)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2023.10.0)\n",
      "Requirement already satisfied: importlib_resources in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.0)\n",
      "Requirement already satisfied: zipp in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: cloudpickle in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/f2/b8/b1ec82fce93064a73ba67f2bb158ec9cac4a0e8f0b6942268ec963947329/regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/81/78/68f6a9421de8d0f936caa39ec0af493c694c2ecdb1784788b6f9f5447cb4/tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/21/5a/9300f8db3714ba8831d2479d47c0bf34393903cd8a90ac4ac09d1d85cc9c/safetensors-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.17.3 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.14.1 tqdm-4.66.1 transformers-4.34.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ml_collections\n",
    "%pip install flax\n",
    "%pip install cloudpickle\n",
    "%pip install einops\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rzhang139/anaconda3/envs/CLIP-ViL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from ml_collections import ConfigDict\n",
    "from ml_collections.config_dict import config_dict\n",
    "import pickle\n",
    "import einops\n",
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PyTorch transformer definition\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, depth, input_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.depth = depth\n",
    "        self.input_norm = input_norm\n",
    "\n",
    "        if self.input_norm:\n",
    "            self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.dense_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(depth)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        if self.input_norm:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            y = self.dense_layers[i](x)\n",
    "            y = F.gelu(y)\n",
    "            y = nn.LayerNorm(y)\n",
    "            if i > 0:\n",
    "                x = x + y\n",
    "            else:\n",
    "                x = y\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, input, deterministic=False):\n",
    "        if deterministic:\n",
    "            return input\n",
    "\n",
    "        keep_prob = 1 - self.dropout_prob\n",
    "        shape = (input.shape[0],) + (1,) * (input.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=torch.float32)\n",
    "        random_tensor = random_tensor.floor()\n",
    "        return input.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class TransformerMLP(nn.Module):\n",
    "    def __init__(self, dim=256, out_dim=256, dropout=0.0, kernel_init=None):\n",
    "        super(TransformerMLP, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.kernel_init = kernel_init if kernel_init is not None else nn.init.xavier_uniform_\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, 4 * dim)\n",
    "        self.fc2 = nn.Linear(4 * dim, out_dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, deterministic=False):\n",
    "        x = self.fc1(inputs)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, use_bias=False, att_drop=0, proj_drop=0, kernel_init=None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_bias = use_bias\n",
    "        self.att_drop = att_drop\n",
    "        self.proj_drop = proj_drop\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.kernel_init = kernel_init if kernel_init is not None else nn.init.xavier_uniform_\n",
    "\n",
    "        self.qkv_linear = nn.Linear(dim, dim * 3, bias=use_bias)\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.att_drop_layer = nn.Dropout(att_drop)\n",
    "        self.proj_drop_layer = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, inputs, deterministic=False, padding_mask=None):\n",
    "        batch, n, channels = inputs.shape\n",
    "        qkv = self.qkv_linear(inputs)\n",
    "        qkv = qkv.view(batch, n, 3, self.num_heads, channels // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            padding_mask = padding_mask.expand(attention.shape)\n",
    "            attention = torch.where(padding_mask > 0, torch.tensor(-1e7), attention)\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.att_drop_layer(attention)\n",
    "\n",
    "        x = torch.matmul(attention, v)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(batch, n, channels)\n",
    "        x = self.fc(x)\n",
    "        x = self.proj_drop_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_heads=8, mlp_ratio=4, att_drop=0.0, drop=0.0, drop_path=0.0):\n",
    "        super(Block, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.att_drop = att_drop\n",
    "        self.drop = drop\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.attention = Attention(emb_dim, num_heads, True, att_drop, drop)\n",
    "        self.drop_path_layer1 = DropPath(drop_path)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.transformer_mlp = TransformerMLP(emb_dim, emb_dim, drop)\n",
    "        self.drop_path_layer2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, inputs, deterministic=False, padding_mask=None):\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x = self.attention(x, deterministic, padding_mask)\n",
    "        x = self.drop_path_layer1(x)\n",
    "        inputs = inputs + x\n",
    "\n",
    "        x = self.layer_norm2(inputs)\n",
    "        x = self.transformer_mlp(x, deterministic)\n",
    "        x = self.drop_path_layer2(x)\n",
    "        return inputs + x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim=1024, depth=24, att_drop=0, drop=0, drop_path=0, num_heads=16, mlp_ratio=4):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.depth = depth\n",
    "        self.att_drop = att_drop\n",
    "        self.drop = drop\n",
    "        self.drop_path = drop_path\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(emb_dim, num_heads, mlp_ratio, att_drop, drop, drop_path)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x, deterministic=False, padding_mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, deterministic, padding_mask)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, length):\n",
    "    return np.expand_dims(\n",
    "        get_1d_sincos_pos_embed_from_grid(\n",
    "            embed_dim, np.arange(length, dtype=np.float32)\n",
    "        ),\n",
    "        0\n",
    "    )\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, length):\n",
    "    grid_size = int(length ** 0.5)\n",
    "    assert grid_size * grid_size == length\n",
    "    def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "        assert embed_dim % 2 == 0\n",
    "        # use half of dimensions to encode grid_h\n",
    "        emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "        emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "        emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "        return emb\n",
    "\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    return np.expand_dims(pos_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model size config\n",
    "def get_transformer_by_config(model_type, config):\n",
    "    if model_type == 'small':\n",
    "        config.emb_dim = 384\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 12\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 6\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'base':\n",
    "        config.emb_dim = 768\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 12\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 12\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'large':\n",
    "        config.emb_dim = 1024\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 24\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'huge':\n",
    "        config.emb_dim = 1280\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 32\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'debug':\n",
    "        config.emb_dim = 1024\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 2\n",
    "        config.dec_depth = 2\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    else:\n",
    "        raise ValueError('Unsupported model type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PyTorch MaskedMultimodalAutoencoder\n",
    "class MaskedMultimodalAutoencoder(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config(updates=None):\n",
    "        config = ConfigDict()\n",
    "        config.model_type = 'small'\n",
    "        config.emb_dim = 1024\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 24\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "\n",
    "        config.output_head_depth = 0\n",
    "        config.att_drop = 0.0\n",
    "        config.drop = 0.0\n",
    "        config.drop_path = 0.0\n",
    "\n",
    "        config.use_type_embedding = True\n",
    "\n",
    "        if updates is not None:\n",
    "            config.update(ConfigDict(updates).copy_and_resolve_references())\n",
    "\n",
    "        if config.model_type is not None:\n",
    "            get_transformer_by_config(config.model_type, config)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def __init__(self, text_vocab_size, config_updates=None):\n",
    "        super(MaskedMultimodalAutoencoder, self).__init__()\n",
    "        self.text_vocab_size = text_vocab_size\n",
    "        self.config = self.get_default_config(config_updates)\n",
    "        assert self.text_vocab_size > 0\n",
    "\n",
    "        self.text_embedding = nn.Embedding(self.text_vocab_size,\n",
    "                                           self.config.emb_dim)\n",
    "        self.text_embedding.weight.data.normal_(0.0, 1.0)\n",
    "        self.image_embedding = nn.Linear(768, self.config.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.image_embedding.weight)\n",
    "\n",
    "        if self.config.use_type_embedding:\n",
    "            self.encoder_image_type_embedding = nn.Parameter(\n",
    "                torch.empty(1, 1, self.config.emb_dim).normal_(0.02)\n",
    "            )\n",
    "            self.encoder_text_type_embedding = nn.Parameter(\n",
    "                torch.empty(1, 1, self.config.emb_dim).normal_(0.02)\n",
    "            )\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.empty(1, 1, self.config.emb_dim).normal_(0.02)\n",
    "        )\n",
    "\n",
    "        self.encoder = Transformer(\n",
    "            emb_dim=self.config.emb_dim,\n",
    "            depth=self.config.depth,\n",
    "            att_drop=self.config.att_drop,\n",
    "            drop=self.config.drop,\n",
    "            drop_path=self.config.drop_path,\n",
    "            num_heads=self.config.num_heads,\n",
    "            mlp_ratio=self.config.mlp_ratio,\n",
    "        )\n",
    "\n",
    "    def get_type_embedding(self, name):\n",
    "        if self.config.use_type_embedding:\n",
    "            return {\n",
    "                'encoder_image_type_embedding': self.encoder_image_type_embedding,\n",
    "                'encoder_text_type_embedding': self.encoder_text_type_embedding,\n",
    "            }[name]\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def forward_representation(self, image, text, text_padding_mask, deterministic=False):\n",
    "        batch_size = image.shape[0]\n",
    "        cls_token = self.cls_token.expand(batch_size, 1, self.config.emb_dim)\n",
    "        input_tensors = [cls_token]\n",
    "        padding_masks = [torch.zeros((batch_size, 1), dtype=torch.float32)]\n",
    "        if image is not None:\n",
    "            image_x = (\n",
    "                self.image_embedding(image)\n",
    "                + get_2d_sincos_pos_embed(self.config.emb_dim, image.shape[1])\n",
    "                + self.get_type_embedding('encoder_image_type_embedding')\n",
    "            )\n",
    "            input_tensors.append(image_x)\n",
    "            padding_masks.append(torch.zeros((batch_size, image.shape[1]), dtype=torch.float32))\n",
    "\n",
    "        if text is not None:\n",
    "            text_x = (\n",
    "                self.text_embedding(text)\n",
    "                + get_1d_sincos_pos_embed(self.config.emb_dim, text.shape[1])\n",
    "                + self.get_type_embedding('encoder_text_type_embedding')\n",
    "            )\n",
    "            input_tensors.append(text_x)\n",
    "            padding_masks.append(text_padding_mask)\n",
    "\n",
    "        x = torch.cat(input_tensors, dim=1)\n",
    "        padding_mask = torch.cat(padding_masks, dim=1)\n",
    "        x = self.encoder(x, deterministic, padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\n",
      "  Using cached pickle5-0.0.11.tar.gz (132 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[96 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/pickle5\n",
      "  \u001b[31m   \u001b[0m copying pickle5/pickletools.py -> build/lib.linux-x86_64-cpython-311/pickle5\n",
      "  \u001b[31m   \u001b[0m copying pickle5/pickle.py -> build/lib.linux-x86_64-cpython-311/pickle5\n",
      "  \u001b[31m   \u001b[0m copying pickle5/__init__.py -> build/lib.linux-x86_64-cpython-311/pickle5\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/pickle5/test\n",
      "  \u001b[31m   \u001b[0m copying pickle5/test/test_picklebuffer.py -> build/lib.linux-x86_64-cpython-311/pickle5/test\n",
      "  \u001b[31m   \u001b[0m copying pickle5/test/test_pickle.py -> build/lib.linux-x86_64-cpython-311/pickle5/test\n",
      "  \u001b[31m   \u001b[0m copying pickle5/test/__init__.py -> build/lib.linux-x86_64-cpython-311/pickle5/test\n",
      "  \u001b[31m   \u001b[0m copying pickle5/test/pickletester.py -> build/lib.linux-x86_64-cpython-311/pickle5/test\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'pickle5._pickle' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/pickle5\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /home/rzhang139/anaconda3/envs/CLIP-ViL/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/rzhang139/anaconda3/envs/CLIP-ViL/include -fPIC -O2 -isystem /home/rzhang139/anaconda3/envs/CLIP-ViL/include -fPIC -I/home/rzhang139/anaconda3/envs/CLIP-ViL/include/python3.11 -c pickle5/_pickle.c -o build/temp.linux-x86_64-cpython-311/pickle5/_pickle.o -std=c99\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_New’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:464:19: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m   464 |     Py_SIZE(self) = 0;\n",
      "  \u001b[31m   \u001b[0m       |                   ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_clear’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:491:19: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m   491 |     Py_SIZE(self) = clearto;\n",
      "  \u001b[31m   \u001b[0m       |                   ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_pop’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:542:23: error: lvalue required as decrement operand\n",
      "  \u001b[31m   \u001b[0m   542 |     return self->data[--Py_SIZE(self)];\n",
      "  \u001b[31m   \u001b[0m       |                       ^~\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_push’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:552:29: error: lvalue required as increment operand\n",
      "  \u001b[31m   \u001b[0m   552 |     self->data[Py_SIZE(self)++] = obj;\n",
      "  \u001b[31m   \u001b[0m       |                             ^~\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_poptuple’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:582:19: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m   582 |     Py_SIZE(self) = start;\n",
      "  \u001b[31m   \u001b[0m       |                   ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_poplist’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:599:19: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m   599 |     Py_SIZE(self) = start;\n",
      "  \u001b[31m   \u001b[0m       |                   ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘_Unpickler_ReadFromFile’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:1285:16: warning: implicit declaration of function ‘_PyObject_CallNoArg’; did you mean ‘PyObject_CallNoArgs’? [-Wimplicit-function-declaration]\n",
      "  \u001b[31m   \u001b[0m  1285 |         data = _PyObject_CallNoArg(self->readline);\n",
      "  \u001b[31m   \u001b[0m       |                ^~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       |                PyObject_CallNoArgs\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:1285:14: warning: assignment to ‘PyObject *’ {aka ‘struct _object *’} from ‘int’ makes pointer from integer without a cast [-Wint-conversion]\n",
      "  \u001b[31m   \u001b[0m  1285 |         data = _PyObject_CallNoArg(self->readline);\n",
      "  \u001b[31m   \u001b[0m       |              ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘whichmodule’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:1937:15: warning: implicit declaration of function ‘_PySys_GetObjectId’; did you mean ‘PySys_GetObject’? [-Wimplicit-function-declaration]\n",
      "  \u001b[31m   \u001b[0m  1937 |     modules = _PySys_GetObjectId(&PyId_modules);\n",
      "  \u001b[31m   \u001b[0m       |               ^~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       |               PySys_GetObject\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:1937:13: warning: assignment to ‘PyObject *’ {aka ‘struct _object *’} from ‘int’ makes pointer from integer without a cast [-Wint-conversion]\n",
      "  \u001b[31m   \u001b[0m  1937 |     modules = _PySys_GetObjectId(&PyId_modules);\n",
      "  \u001b[31m   \u001b[0m       |             ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘save_float’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:2255:13: warning: implicit declaration of function ‘_PyFloat_Pack8’; did you mean ‘PyFloat_Pack8’? [-Wimplicit-function-declaration]\n",
      "  \u001b[31m   \u001b[0m  2255 |         if (_PyFloat_Pack8(x, (unsigned char *)&pdata[1], 0) < 0)\n",
      "  \u001b[31m   \u001b[0m       |             ^~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       |             PyFloat_Pack8\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘save’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:4452:30: warning: assignment to ‘PyObject *’ {aka ‘struct _object *’} from ‘int’ makes pointer from integer without a cast [-Wint-conversion]\n",
      "  \u001b[31m   \u001b[0m  4452 |                 reduce_value = _PyObject_CallNoArg(reduce_func);\n",
      "  \u001b[31m   \u001b[0m       |                              ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘load_binfloat’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:5428:9: warning: implicit declaration of function ‘_PyFloat_Unpack8’; did you mean ‘PyFloat_Unpack8’? [-Wimplicit-function-declaration]\n",
      "  \u001b[31m   \u001b[0m  5428 |     x = _PyFloat_Unpack8((unsigned char *)s, 0);\n",
      "  \u001b[31m   \u001b[0m       |         ^~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m       |         PyFloat_Unpack8\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘load_pop’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:6178:30: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m  6178 |         Py_SIZE(self->stack) = len;\n",
      "  \u001b[31m   \u001b[0m       |                              ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘do_append’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:6535:42: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m  6535 |                     Py_SIZE(self->stack) = x;\n",
      "  \u001b[31m   \u001b[0m       |                                          ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:6541:34: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m  6541 |             Py_SIZE(self->stack) = x;\n",
      "  \u001b[31m   \u001b[0m       |                                  ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘load_additems’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:6663:38: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m  6663 |                 Py_SIZE(self->stack) = mark;\n",
      "  \u001b[31m   \u001b[0m       |                                      ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:6668:30: error: lvalue required as left operand of assignment\n",
      "  \u001b[31m   \u001b[0m  6668 |         Py_SIZE(self->stack) = mark;\n",
      "  \u001b[31m   \u001b[0m       |                              ^\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c: In function ‘Pdata_pop’:\n",
      "  \u001b[31m   \u001b[0m pickle5/_pickle.c:543:1: warning: control reaches end of non-void function [-Wreturn-type]\n",
      "  \u001b[31m   \u001b[0m   543 | }\n",
      "  \u001b[31m   \u001b[0m       | ^\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pickle5\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pickle5\n",
      "Failed to build pickle5\n",
      "\u001b[31mERROR: Could not build wheels for pickle5, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "code() argument 13 must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rzhang139/p1/m3ae_public/m3ae.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B128.32.43.234/home/rzhang139/p1/m3ae_public/m3ae.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# load Jax pretrained weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B128.32.43.234/home/rzhang139/p1/m3ae_public/m3ae.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(model_path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B128.32.43.234/home/rzhang139/p1/m3ae_public/m3ae.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     pretrained \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B128.32.43.234/home/rzhang139/p1/m3ae_public/m3ae.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m jax_weights \u001b[39m=\u001b[39m pretrained[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B128.32.43.234/home/rzhang139/p1/m3ae_public/m3ae.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m jax_config \u001b[39m=\u001b[39m pretrained[\u001b[39m'\u001b[39m\u001b[39mvariant\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: code() argument 13 must be str, not int"
     ]
    }
   ],
   "source": [
    "# @title Choose model size and load pretrained weights\n",
    "# import pickle5 as pickle\n",
    "\n",
    "model_type = 'small' #@param [\"small\", \"base\", \"large\"]\n",
    "model_path = './m3ae_small.pkl'\n",
    "model_config = ConfigDict(dict(model_type='small'))\n",
    "model = MaskedMultimodalAutoencoder(text_vocab_size=30522, config_updates=model_config)\n",
    "model = model.cuda()\n",
    "\n",
    "# load Jax pretrained weights\n",
    "with open(model_path, 'rwb') as f:\n",
    "    pretrained = pickle.load(f)\n",
    "jax_weights = pretrained['state']\n",
    "jax_config = pretrained['variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# @title Replace PyTorch weight with Jax pretrained models weights\n",
    "for n, p in model.named_parameters():\n",
    "    if n in ['cls_token', 'encoder_image_type_embedding', 'encoder_text_type_embedding']:\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params'][n]))\n",
    "    elif n == 'image_embedding.weight':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['image_embedding']['kernel']).t())\n",
    "    elif n == 'image_embedding.bias':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['image_embedding']['bias']).t())\n",
    "    elif n == 'text_embedding.weight':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['text_embedding']['embedding']))\n",
    "    elif n == 'encoder.layer_norm.weight':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['encoder']['LayerNorm_0']['scale']))\n",
    "    elif n == 'encoder.layer_norm.bias':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['encoder']['LayerNorm_0']['bias']))\n",
    "    elif n.startswith('encoder.blocks.'):\n",
    "        block_num = n.split('encoder.blocks.')[1].split('.')[0]\n",
    "        jax_block_weights = jax_weights.params['params']['encoder'][f\"Block_{block_num}\"]\n",
    "        if n == f\"encoder.blocks.{block_num}.layer_norm1.weight\":\n",
    "            v = jax_block_weights['LayerNorm_0']['scale']\n",
    "        elif n == f\"encoder.blocks.{block_num}.layer_norm1.bias\":\n",
    "            v = jax_block_weights['LayerNorm_0']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.layer_norm2.weight\":\n",
    "            v = jax_block_weights['LayerNorm_1']['scale']\n",
    "        elif n == f\"encoder.blocks.{block_num}.layer_norm2.bias\":\n",
    "            v = jax_block_weights['LayerNorm_1']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.qkv_linear.weight\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_0']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.qkv_linear.bias\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_0']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.fc.weight\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_1']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.fc.bias\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_1']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc1.weight\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc1']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc1.bias\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc1']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc2.weight\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc2']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc2.bias\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc2']['bias']\n",
    "        else:\n",
    "            raise False\n",
    "        p.data.copy_(torch.from_numpy(v).t())\n",
    "    else:\n",
    "        raise False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# @title Forward representation\n",
    "\n",
    "caption = [\"hello\", \"how are you\"]\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_caption = tokenizer(\n",
    "    caption,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "tokenized_caption = torch.from_numpy(encoded_caption[\"input_ids\"][0])[None, ...]\n",
    "padding_mask = 1.0 - encoded_caption[\"attention_mask\"][0].astype(np.float32)\n",
    "padding_mask = torch.from_numpy(padding_mask[None, ...])\n",
    "\n",
    "images = torch.zeros((1, 3, 256, 256))\n",
    "patch_size = 16\n",
    "image_patches = einops.rearrange(images,\n",
    "        'b c (h p1) (w p2) -> b (h w) (c p1 p2)',\n",
    "        p1=patch_size,\n",
    "        p2=patch_size)\n",
    "print(image_patches.shape)\n",
    "with torch.no_grad():\n",
    "  # no masking, just encoders\n",
    "    representation = model.forward_representation(image_patches, tokenized_caption, padding_mask)\n",
    "    # decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "representation.shape\n",
    "# 1 sample with 513 tokens each having a 384-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# @title Linear classifier: we are training it with representations\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as pth_transforms\n",
    "from einops import rearrange\n",
    "\n",
    " # ============ preparing samples ... ============\n",
    "def rearrange_patches(image, patch_size=16):\n",
    "    patches = rearrange(image, 'c (h p1) (w p2) -> (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\n",
    "    # Calculate the padding needed\n",
    "    target_shape = (256, 768)\n",
    "    pad_h = target_shape[0] - patches.shape[0]\n",
    "    pad_w = target_shape[1] - patches.shape[1]\n",
    "    \n",
    "    # Apply padding\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        patches = F.pad(patches, (0, pad_w, 0, pad_h), 'constant', 0)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "train_transform = pth_transforms.Compose([\n",
    "  pth_transforms.Resize(256, interpolation=3),\n",
    "  pth_transforms.CenterCrop(224),\n",
    "  pth_transforms.ToTensor(),\n",
    "  pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "  pth_transforms.Lambda(lambda img: rearrange_patches(img))\n",
    "])\n",
    "dataset_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "# cifar100_training = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "sampler = torch.utils.data.distributed.DistributedSampler(dataset_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    sampler=sampler,\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Text\n",
    "caption = \"\"\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_caption = tokenizer(\n",
    "    caption,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "tokenized_caption = torch.from_numpy(encoded_caption[\"input_ids\"][0])[None, ...]\n",
    "padding_mask = 1.0 - encoded_caption[\"attention_mask\"][0].astype(np.float32)\n",
    "padding_mask = torch.from_numpy(padding_mask[None, ...])\n",
    "\n",
    "representations = []\n",
    "with torch.no_grad():\n",
    "    for batchIdx, (images, labels) in enumerate(train_loader):\n",
    "            representation = model.forward_representation(images.cuda(), tokenized_caption.cuda(), padding_mask.cuda())\n",
    "            representations.append((representation, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "representations_tensor = torch.stack(representations)\n",
    "\n",
    "# Save to .pt file\n",
    "torch.save(representations_tensor, 'representations.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "r = torch.load('representations.pt')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Autoencoder: output image given text, take pair, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'm3ae' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n m3ae ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Future tasks for autoencoders: generation and reconstruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
