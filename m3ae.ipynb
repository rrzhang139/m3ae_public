{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml_collections in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (0.1.1)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from ml_collections) (1.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from ml_collections) (6.0.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from ml_collections) (1.16.0)\n",
      "Requirement already satisfied: contextlib2 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from ml_collections) (21.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (4.19.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2022.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/m3ae/lib/python3.8/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ml_collections\n",
    "%pip install einops\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from ml_collections import ConfigDict\n",
    "from ml_collections.config_dict import config_dict\n",
    "import pickle\n",
    "import einops\n",
    "import pprint\n",
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PyTorch transformer definition\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, depth, input_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.depth = depth\n",
    "        self.input_norm = input_norm\n",
    "\n",
    "        if self.input_norm:\n",
    "            self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.dense_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(depth)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        if self.input_norm:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            y = self.dense_layers[i](x)\n",
    "            y = F.gelu(y)\n",
    "            y = nn.LayerNorm(y)\n",
    "            if i > 0:\n",
    "                x = x + y\n",
    "            else:\n",
    "                x = y\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, input, deterministic=False):\n",
    "        if deterministic:\n",
    "            return input\n",
    "\n",
    "        keep_prob = 1 - self.dropout_prob\n",
    "        shape = (input.shape[0],) + (1,) * (input.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=torch.float32)\n",
    "        random_tensor = random_tensor.floor()\n",
    "        return input.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class TransformerMLP(nn.Module):\n",
    "    def __init__(self, dim=256, out_dim=256, dropout=0.0, kernel_init=None):\n",
    "        super(TransformerMLP, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.kernel_init = kernel_init if kernel_init is not None else nn.init.xavier_uniform_\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, 4 * dim)\n",
    "        self.fc2 = nn.Linear(4 * dim, out_dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, deterministic=False):\n",
    "        x = self.fc1(inputs)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, use_bias=False, att_drop=0, proj_drop=0, kernel_init=None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_bias = use_bias\n",
    "        self.att_drop = att_drop\n",
    "        self.proj_drop = proj_drop\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.kernel_init = kernel_init if kernel_init is not None else nn.init.xavier_uniform_\n",
    "\n",
    "        self.qkv_linear = nn.Linear(dim, dim * 3, bias=use_bias)\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.att_drop_layer = nn.Dropout(att_drop)\n",
    "        self.proj_drop_layer = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, inputs, deterministic=False, padding_mask=None):\n",
    "        batch, n, channels = inputs.shape\n",
    "        qkv = self.qkv_linear(inputs)\n",
    "        qkv = qkv.view(batch, n, 3, self.num_heads, channels // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            padding_mask = padding_mask.expand(attention.shape)\n",
    "            attention = torch.where(padding_mask > 0, torch.tensor(-1e7), attention)\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.att_drop_layer(attention)\n",
    "\n",
    "        x = torch.matmul(attention, v)\n",
    "        x = x.permute(0, 2, 1, 3).reshape(batch, n, channels)\n",
    "        x = self.fc(x)\n",
    "        x = self.proj_drop_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_heads=8, mlp_ratio=4, att_drop=0.0, drop=0.0, drop_path=0.0):\n",
    "        super(Block, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.att_drop = att_drop\n",
    "        self.drop = drop\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.attention = Attention(emb_dim, num_heads, True, att_drop, drop)\n",
    "        self.drop_path_layer1 = DropPath(drop_path)\n",
    "        self.layer_norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.transformer_mlp = TransformerMLP(emb_dim, emb_dim, drop)\n",
    "        self.drop_path_layer2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, inputs, deterministic=False, padding_mask=None):\n",
    "        x = self.layer_norm1(inputs)\n",
    "        x = self.attention(x, deterministic, padding_mask)\n",
    "        x = self.drop_path_layer1(x)\n",
    "        inputs = inputs + x\n",
    "\n",
    "        x = self.layer_norm2(inputs)\n",
    "        x = self.transformer_mlp(x, deterministic)\n",
    "        x = self.drop_path_layer2(x)\n",
    "        return inputs + x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim=1024, depth=24, att_drop=0, drop=0, drop_path=0, num_heads=16, mlp_ratio=4):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.depth = depth\n",
    "        self.att_drop = att_drop\n",
    "        self.drop = drop\n",
    "        self.drop_path = drop_path\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(emb_dim, num_heads, mlp_ratio, att_drop, drop, drop_path)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x, deterministic=False, padding_mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, deterministic, padding_mask)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, length):\n",
    "    return np.expand_dims(\n",
    "        get_1d_sincos_pos_embed_from_grid(\n",
    "            embed_dim, np.arange(length, dtype=np.float32)\n",
    "        ),\n",
    "        0\n",
    "    )\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, length):\n",
    "    grid_size = int(length ** 0.5)\n",
    "    assert grid_size * grid_size == length\n",
    "    def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "        assert embed_dim % 2 == 0\n",
    "        # use half of dimensions to encode grid_h\n",
    "        emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "        emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "        emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "        return emb\n",
    "\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    return np.expand_dims(pos_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model size config\n",
    "def get_transformer_by_config(model_type, config):\n",
    "    if model_type == 'small':\n",
    "        config.emb_dim = 384\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 12\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 6\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'base':\n",
    "        config.emb_dim = 768\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 12\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 12\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'large':\n",
    "        config.emb_dim = 1024\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 24\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'huge':\n",
    "        config.emb_dim = 1280\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 32\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    elif model_type == 'debug':\n",
    "        config.emb_dim = 1024\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 2\n",
    "        config.dec_depth = 2\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "    else:\n",
    "        raise ValueError('Unsupported model type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PyTorch MaskedMultimodalAutoencoder\n",
    "class MaskedMultimodalAutoencoder(nn.Module):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config(updates=None):\n",
    "        config = ConfigDict()\n",
    "        config.model_type = 'small'\n",
    "        config.emb_dim = 1024\n",
    "        config.dec_emb_dim = 512\n",
    "        config.depth = 24\n",
    "        config.dec_depth = 8\n",
    "        config.num_heads = 16\n",
    "        config.dec_num_heads = 16\n",
    "        config.mlp_ratio = 4\n",
    "\n",
    "        config.output_head_depth = 0\n",
    "        config.att_drop = 0.0\n",
    "        config.drop = 0.0\n",
    "        config.drop_path = 0.0\n",
    "\n",
    "        config.use_type_embedding = True\n",
    "\n",
    "        if updates is not None:\n",
    "            config.update(ConfigDict(updates).copy_and_resolve_references())\n",
    "\n",
    "        if config.model_type is not None:\n",
    "            get_transformer_by_config(config.model_type, config)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def __init__(self, text_vocab_size, config_updates=None):\n",
    "        super(MaskedMultimodalAutoencoder, self).__init__()\n",
    "        self.text_vocab_size = text_vocab_size\n",
    "        self.config = self.get_default_config(config_updates)\n",
    "        assert self.text_vocab_size > 0\n",
    "\n",
    "        self.text_embedding = nn.Embedding(self.text_vocab_size,\n",
    "                                           self.config.emb_dim)\n",
    "        self.text_embedding.weight.data.normal_(0.0, 1.0)\n",
    "        self.image_embedding = nn.Linear(768, self.config.emb_dim)\n",
    "        nn.init.xavier_uniform_(self.image_embedding.weight)\n",
    "\n",
    "        if self.config.use_type_embedding:\n",
    "            self.encoder_image_type_embedding = nn.Parameter(\n",
    "                torch.empty(1, 1, self.config.emb_dim).normal_(0.02)\n",
    "            )\n",
    "            self.encoder_text_type_embedding = nn.Parameter(\n",
    "                torch.empty(1, 1, self.config.emb_dim).normal_(0.02)\n",
    "            )\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.empty(1, 1, self.config.emb_dim).normal_(0.02)\n",
    "        )\n",
    "\n",
    "        self.encoder = Transformer(\n",
    "            emb_dim=self.config.emb_dim,\n",
    "            depth=self.config.depth,\n",
    "            att_drop=self.config.att_drop,\n",
    "            drop=self.config.drop,\n",
    "            drop_path=self.config.drop_path,\n",
    "            num_heads=self.config.num_heads,\n",
    "            mlp_ratio=self.config.mlp_ratio,\n",
    "        )\n",
    "\n",
    "    def get_type_embedding(self, name):\n",
    "        if self.config.use_type_embedding:\n",
    "            return {\n",
    "                'encoder_image_type_embedding': self.encoder_image_type_embedding,\n",
    "                'encoder_text_type_embedding': self.encoder_text_type_embedding,\n",
    "            }[name]\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def forward_representation(self, image, text, text_padding_mask, deterministic=False):\n",
    "        batch_size = image.shape[0]\n",
    "        cls_token = self.cls_token.expand(batch_size, 1, self.config.emb_dim)\n",
    "        input_tensors = [cls_token]\n",
    "        padding_masks = [torch.zeros((batch_size, 1), dtype=torch.float32)]\n",
    "        if image is not None:\n",
    "            image_x = (\n",
    "                self.image_embedding(image)\n",
    "                + get_2d_sincos_pos_embed(self.config.emb_dim, image.shape[1])\n",
    "                + self.get_type_embedding('encoder_image_type_embedding')\n",
    "            )\n",
    "            input_tensors.append(image_x)\n",
    "            padding_masks.append(torch.zeros((batch_size, image.shape[1]), dtype=torch.float32))\n",
    "\n",
    "        if text is not None:\n",
    "            text_x = (\n",
    "                self.text_embedding(text)\n",
    "                + get_1d_sincos_pos_embed(self.config.emb_dim, text.shape[1])\n",
    "                + self.get_type_embedding('encoder_text_type_embedding')\n",
    "            )\n",
    "            input_tensors.append(text_x)\n",
    "            padding_masks.append(text_padding_mask)\n",
    "\n",
    "        x = torch.cat(input_tensors, dim=1)\n",
    "        padding_mask = torch.cat(padding_masks, dim=1)\n",
    "        x = self.encoder(x, deterministic, padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/m3ae/lib/python3.8/site-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: ConnectionError(MaxRetryError(\"HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/agent-worker-number (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa8d9e362e0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\"))\n",
      " This a JAX bug; please report an issue at https://github.com/google/jax/issues\n",
      "  _warn(f\"cloud_tpu_init failed: {repr(exc)}\\n This a JAX bug; please report \"\n"
     ]
    }
   ],
   "source": [
    "# @title Choose model size and load pretrained weights\n",
    "\n",
    "model_type = 'small' #@param [\"small\", \"base\", \"large\"]\n",
    "model_path = './m3ae_small.pkl'\n",
    "model_config = ConfigDict(dict(model_type='small'))\n",
    "model = MaskedMultimodalAutoencoder(text_vocab_size=30522, config_updates=model_config)\n",
    "\n",
    "# load Jax pretrained weights\n",
    "with open(model_path, 'rb') as f:\n",
    "    pretrained = pickle.load(f)\n",
    "jax_weights = pretrained['state']\n",
    "jax_config = pretrained['variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/zh9llzwd25dcvkzv_jl01kqc0000gn/T/ipykernel_97380/3063840937.py:4: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  p.data.copy_(torch.from_numpy(jax_weights.params['params'][n]))\n"
     ]
    }
   ],
   "source": [
    "# @title Replace PyTorch weight with Jax pretrained models weights\n",
    "for n, p in model.named_parameters():\n",
    "    if n in ['cls_token', 'encoder_image_type_embedding', 'encoder_text_type_embedding']:\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params'][n]))\n",
    "    elif n == 'image_embedding.weight':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['image_embedding']['kernel']).t())\n",
    "    elif n == 'image_embedding.bias':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['image_embedding']['bias']).t())\n",
    "    elif n == 'text_embedding.weight':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['text_embedding']['embedding']))\n",
    "    elif n == 'encoder.layer_norm.weight':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['encoder']['LayerNorm_0']['scale']))\n",
    "    elif n == 'encoder.layer_norm.bias':\n",
    "        p.data.copy_(torch.from_numpy(jax_weights.params['params']['encoder']['LayerNorm_0']['bias']))\n",
    "    elif n.startswith('encoder.blocks.'):\n",
    "        block_num = n.split('encoder.blocks.')[1].split('.')[0]\n",
    "        jax_block_weights = jax_weights.params['params']['encoder'][f\"Block_{block_num}\"]\n",
    "        if n == f\"encoder.blocks.{block_num}.layer_norm1.weight\":\n",
    "            v = jax_block_weights['LayerNorm_0']['scale']\n",
    "        elif n == f\"encoder.blocks.{block_num}.layer_norm1.bias\":\n",
    "            v = jax_block_weights['LayerNorm_0']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.layer_norm2.weight\":\n",
    "            v = jax_block_weights['LayerNorm_1']['scale']\n",
    "        elif n == f\"encoder.blocks.{block_num}.layer_norm2.bias\":\n",
    "            v = jax_block_weights['LayerNorm_1']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.qkv_linear.weight\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_0']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.qkv_linear.bias\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_0']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.fc.weight\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_1']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.attention.fc.bias\":\n",
    "            v = jax_block_weights['Attention_0']['Dense_1']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc1.weight\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc1']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc1.bias\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc1']['bias']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc2.weight\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc2']['kernel']\n",
    "        elif n == f\"encoder.blocks.{block_num}.transformer_mlp.fc2.bias\":\n",
    "            v = jax_block_weights['TransformerMLP_0']['fc2']['bias']\n",
    "        else:\n",
    "            raise False\n",
    "        p.data.copy_(torch.from_numpy(v).t())\n",
    "    else:\n",
    "        raise False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# @title Forward representation\n",
    "\n",
    "caption = [\"hello\", \"how are you\"]\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_caption = tokenizer(\n",
    "    caption,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"np\",\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "tokenized_caption = torch.from_numpy(encoded_caption[\"input_ids\"][0])[None, ...]\n",
    "padding_mask = 1.0 - encoded_caption[\"attention_mask\"][0].astype(np.float32)\n",
    "padding_mask = torch.from_numpy(padding_mask[None, ...])\n",
    "\n",
    "images = torch.zeros((1, 3, 256, 256))\n",
    "patch_size = 16\n",
    "image_patches = einops.rearrange(images,\n",
    "        'b c (h p1) (w p2) -> b (h w) (c p1 p2)',\n",
    "        p1=patch_size,\n",
    "        p2=patch_size)\n",
    "print(image_patches.shape)\n",
    "with torch.no_grad():\n",
    "  # no masking, just encoders\n",
    "    representation = model.forward_representation(image_patches, tokenized_caption, padding_mask)\n",
    "    # decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 513, 384])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation.shape\n",
    "# 1 sample with 513 tokens each having a 384-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Linear classifier\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as pth_transforms\n",
    "from einops import rearrange\n",
    "\n",
    " # ============ preparing samples ... ============\n",
    "def rearrange_patches(image, patch_size=16):\n",
    "    return rearrange(image, 'c (h p1) (w p2) -> (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\n",
    "\n",
    "train_transform = pth_transforms.Compose([\n",
    "  pth_transforms.Resize(256, interpolation=3),\n",
    "  pth_transforms.CenterCrop(224),\n",
    "  pth_transforms.ToTensor(),\n",
    "  pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "  pth_transforms.Lambda(lambda img: rearrange_patches(img))\n",
    "])\n",
    "dataset_val = datasets.ImageFolder(os.path.join(\"./tiny-imagenet-200\", \"train\"), transform=train_transform)\n",
    "dataset_val[0][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder: output image given text, take pair, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future tasks for autoencoders: generation and reconstruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
